{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment $2$\n",
    "\n",
    "*Neelay Upadhyaya __18210053__*\n",
    "\n",
    "*M.Tech CSE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download any of these text books from Project Gutenberg \n",
    "\n",
    "Text used: Sherlock Holmes: The Adventures of Sherlock Holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "\n",
    "corpus = open('sherlock.txt', 'r')\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(corpus.read())\n",
    "\n",
    "# cleaning and pre-processing data.\n",
    "sentences = ['<s> ' + word.strip().replace('\\n', ' ').lower() + ' </s>' for word in sentences]\n",
    "\n",
    "#random.shuffle(sentences)\n",
    "training_index = int(len(sentences) * 0.8)\n",
    "sentences_training = sentences[:training_index]\n",
    "sentences_test = sentences[training_index:]\n",
    "\n",
    "print('Total sentences:', len(sentences))\n",
    "print('Training sentences:', len(sentences_training))\n",
    "print('Training sentences:', len(sentences_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class has all the functionalities required for n-grams.\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class NGrams:\n",
    "    def __init__(self, sentences=[]):\n",
    "        self.sentences = sentences\n",
    "        self.master_dictionary = {}\n",
    "        self.mle_dictionary = {}\n",
    "        \n",
    "        \n",
    "    def __ngrams(self, n):\n",
    "        dictionary = {}\n",
    "        for sentence in self.sentences:\n",
    "            words = [word for word in sentence.split()]\n",
    "            for i in range(0, len(words) - n + 1):\n",
    "                ngram = ' '.join(words[i : i + n])\n",
    "                frequency = dictionary.get(ngram, 0)\n",
    "                dictionary[ngram] = frequency + 1\n",
    "        \n",
    "        self.master_dictionary[n] = dictionary\n",
    "        return self.master_dictionary[n]\n",
    "    \n",
    "    \n",
    "    def unigrams(self):\n",
    "        return self.__ngrams(1)\n",
    "    \n",
    "    \n",
    "    def bigrams(self):\n",
    "        return self.__ngrams(2)\n",
    "    \n",
    "    \n",
    "    def trigrams(self):\n",
    "        return self.__ngrams(3)\n",
    "    \n",
    "    \n",
    "    def quadgrams(self):\n",
    "        return self.__ngrams(4)\n",
    "    \n",
    "    \n",
    "    def __compute_ngram_mle(self, n):\n",
    "        if n == 1:  \n",
    "            denominator = sum(self.master_dictionary[n].values())\n",
    "        \n",
    "        dictionary = {}\n",
    "        for ngram_word, freq in self.master_dictionary[n].items():\n",
    "            words = ngram_word.split()\n",
    "            \n",
    "            if n != 1:\n",
    "                denominator = self.master_dictionary[n - 1][' '.join(words[:-1])]\n",
    "\n",
    "            probability = freq / denominator\n",
    "            dictionary[ngram_word] = probability\n",
    "            \n",
    "        self.mle_dictionary[n] = dictionary\n",
    "        return self.mle_dictionary[n]\n",
    "    \n",
    "        \n",
    "    def compute_unigram_mle(self):\n",
    "        return self.__compute_ngram_mle(1)    \n",
    "    \n",
    "    \n",
    "    def compute_bigram_mle(self):\n",
    "        return self.__compute_ngram_mle(2)\n",
    "        \n",
    "        \n",
    "    def compute_trigram_mle(self):\n",
    "        return self.__compute_ngram_mle(3)\n",
    "    \n",
    "    \n",
    "    def compute_quadgram_mle(self):\n",
    "        return self.__compute_ngram_mle(4)\n",
    "        \n",
    "    \n",
    "    def find_probability_of_sentence(self, sentence, n):\n",
    "        sentence = '<s> ' + sentence + ' </s>'\n",
    "        words = sentence.split()\n",
    "        probability = 0\n",
    "        \n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = ' '.join(words[i : i + n])\n",
    "            print(ngram)\n",
    "            mle = self.mle_dictionary[n].get(ngram, 0)\n",
    "            print(mle)\n",
    "            if mle == 0:\n",
    "                return 'Cannot find the probability of the sentence, as the corpus has no \"' + ngram + '\"'\n",
    "            probability += math.log(mle)\n",
    "            \n",
    "        return probability\n",
    "            \n",
    "    def __random_next(self, n, given_word):\n",
    "        current_dictionary = self.master_dictionary[n]\n",
    "        \n",
    "        words = list(current_dictionary.keys())\n",
    "        given_candidates = [word for word in words if word.startswith(given_word)]\n",
    "        \n",
    "        if len(given_candidates) == 0:\n",
    "            return '</s>'\n",
    "        \n",
    "        candidate_probabilites = []\n",
    "        for i in range(len(given_candidates)):\n",
    "            candidate_probabilites.append(self.mle_dictionary[n].get(given_candidates[i], 0))\n",
    "        \n",
    "        normalization_factor = sum(candidate_probabilites)\n",
    "        \n",
    "        for index, probability in enumerate(candidate_probabilites):\n",
    "            candidate_probabilites[index] = probability / normalization_factor\n",
    "        \n",
    "        # print(candidate_probabilites[:20])\n",
    "        outcomes = list(np.random.multinomial(100, candidate_probabilites))\n",
    "        max_value = max(outcomes)\n",
    "        max_index = outcomes.index(max_value)\n",
    "        \n",
    "        return ' '.join(given_candidates[max_index].split()[1:])\n",
    "            \n",
    "    def generate_sentence(self, model):\n",
    "        sentence = '<s> '\n",
    "        next_word = self.__random_next(model, '<s>')\n",
    "        \n",
    "        sentence += next_word + ' '\n",
    "    \n",
    "        while(not next_word.endswith('</s>')):\n",
    "            next_word = next_word.split()\n",
    "            next_word = self.__random_next(model, next_word[-1])\n",
    "            sentence += next_word + ' '\n",
    "        \n",
    "        return sentence.strip()\n",
    "\n",
    "#ngram = NGrams([\"I am Sam\", \"Sam I am\", \"I do not like green eggs and ham\"])\n",
    "ngram = NGrams(sentences_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ngram.unigrams()\n",
    "bigrams = ngram.bigrams()\n",
    "trigrams = ngram.trigrams()\n",
    "quadgrams = ngram.quadgrams()\n",
    "\n",
    "for key, value in bigrams.items():\n",
    "    print(key, \" \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE for unigram, bigram, trigrams and quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mle = ngram.compute_unigram_mle()\n",
    "bigram_mle = ngram.compute_bigram_mle()\n",
    "trigram_mle = ngram.compute_trigram_mle()\n",
    "quadgram_mle = ngram.compute_quadgram_mle()\n",
    "\n",
    "for key, value in bigram_mle.items():\n",
    "    print(key, \" \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max possible N-grams Vs Actual N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = len(unigrams)\n",
    "\n",
    "print('Total types ', v)\n",
    "\n",
    "print('Total possible bigrams count: ', v ** 2)\n",
    "print('Total possible trigrams count: ', v ** 3)\n",
    "print('Total possible quadgrams count: ', v ** 4)\n",
    "\n",
    "print('Total actual bigrams count:', len(bigrams))\n",
    "print('Total actual trigrams count:', len(trigrams))\n",
    "print('Total actual quadgrams count:', len(quadgrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_type(model):\n",
    "    return model.isnumeric() and int(model) in [1, 2, 3, 4]\n",
    "\n",
    "def validate_number_of_sentences(number):\n",
    "    return number.isnumeric() and 2 <= int(number) <= 10\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Generation according to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = input(\"Enter a model for which you want to generate sentences: \\n 1 for unigram \\n 2 for bigram \\n 3 for trigram \\n 4 for quadgram \\n\")\n",
    "\n",
    "number_of_sentences = input(\"Enter how many sentences you want? Enter between 2-10 inclusive \\n\")\n",
    "\n",
    "if validate_model_type(model_type) and validate_number_of_sentences(number_of_sentences):\n",
    "    for i in range(int(number_of_sentences)):\n",
    "        print(ngram.generate_sentence(int(model_type)))\n",
    "else:\n",
    "    print(\"You did not enter a valid input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the probability of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = input(\"Enter a sentence for which you want to find probability: \\n\")\n",
    "\n",
    "model_type = input(\"Enter a model for which you want to generate sentences: \\n 1 for unigram \\n 2 for bigram \\n 3 for trigram \\n 4 for quadgram \\n\")\n",
    "\n",
    "if validate_model_type(model_type):\n",
    "    probability = ngram.find_probability_of_sentence(input_sentence.lower(), int(model_type))\n",
    "    print(probability)\n",
    "    \n",
    "else:\n",
    "    print(\"You did not enter a valid input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class provides the required functionalities for smoothing.\n",
    "class Smoothing:\n",
    "    \n",
    "    def add_one(self, bigrams, unigrams):\n",
    "        new_bigrams = {}\n",
    "        new_bigram_mle = {}\n",
    "        v = len(unigrams)\n",
    "        for ngram, freq in bigrams.items():\n",
    "            words = ngram.split()\n",
    "            denominator = unigrams.get(words[0], 0) + v\n",
    "            probability = (freq + 1) / denominator\n",
    "            new_bigram_mle[ngram] = probability\n",
    "            new_bigrams[ngram] = probability * unigrams.get(words[0], 0)\n",
    "            \n",
    "        return new_bigrams, new_bigram_mle\n",
    "    \n",
    "    def good_turing(self, unigrams):\n",
    "        frequency_of_frequency = {}\n",
    "        for value in unigrams.values():\n",
    "            frequency = frequency_of_frequency.get(value, 0)\n",
    "            if(frequency == 0):\n",
    "                frequency_of_frequency[value] = 1\n",
    "            else: \n",
    "                frequency_of_frequency[value] += 1\n",
    "                \n",
    "        v = len(unigrams)\n",
    "        \n",
    "        good_turing_counts = {}\n",
    "        good_turing_counts[0] = frequency_of_frequency.get(1) / v\n",
    "        \n",
    "        for i in range(1, 11):\n",
    "            good_turing_counts[i] = (i + 1) * frequency_of_frequency.get(i + 1, 0) / frequency_of_frequency.get(i, 0)\n",
    "        \n",
    "        return good_turing_counts\n",
    "    \n",
    "    def good_turing_smoothing(self, d):\n",
    "        new_bigrams = {}\n",
    "        new_bigram_mle = {}\n",
    "        v = len(unigrams)\n",
    "        for ngram, freq in bigrams.items():\n",
    "            words = ngram.split()\n",
    "            denominator = unigrams.get(words[0], 0) + v\n",
    "            probability = (freq - d) / denominator\n",
    "            new_bigram_mle[ngram] = probability\n",
    "            new_bigrams[ngram] = probability * unigrams.get(words[0], 0)\n",
    "            \n",
    "        return new_bigrams, new_bigram_mle\n",
    "    \n",
    "    def get_good_turing_discount_value(self, good_turing_counts):\n",
    "        differences = []\n",
    "        for key, value in good_turing_counts.items():\n",
    "            differences.append(abs(key - value))\n",
    "        \n",
    "        #print(differences)\n",
    "        return sum(differences) / float(len(differences))\n",
    "        \n",
    "smoothing = Smoothing()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one_smoothing = smoothing.add_one(bigrams, unigrams)\n",
    "new_bigrams_add_one = add_one_smoothing[0]\n",
    "new_bigrams_mle = add_one_smoothing[1]\n",
    "\n",
    "for k, v in new_bigrams_mle.items():\n",
    "    print(k, \" \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples where add-1 smoothing causes drastic changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original bigram count for \"for a\":',bigrams['for a'])\n",
    "print('New bigram count for \"for a\":', new_bigrams_add_one['for a'])\n",
    "\n",
    "print('Original bigram count for \"when he\":', bigrams['when he'])\n",
    "print('New bigram count for \"when he\":',new_bigrams_add_one['when he'])\n",
    "\n",
    "print('Original bigram count for \"of her\":',bigrams['of her'])\n",
    "print('New bigram count for \"of her\":',new_bigrams_add_one['of her'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there are counts which have decreased by a factor of 8-20. \n",
    "\n",
    "The reason for this drastic change is that a lot of probability mass is moved to the bigrams with count 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount value by implementing Good-Turing smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_turing_counts = smoothing.good_turing(unigrams)\n",
    "d = smoothing.get_good_turing_discount_value(good_turing_counts)\n",
    "print('Constant Discount value(d): ', d)\n",
    "\n",
    "\n",
    "good_turing_smoothing = smoothing.good_turing_smoothing(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity for add-1 and Good-Turing smoothing on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    \n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        \n",
    "        \n",
    "    def add_one(self, bigrams, unigrams):\n",
    "        smoothed_bigrams = {}\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split()\n",
    "            for index in range(len(words) - 1):\n",
    "                ngram = words[index] + ' ' + words[index + 1]\n",
    "                #print(ngram)\n",
    "                if ngram not in bigrams:\n",
    "                    denominator = unigrams.get(words[index], 0) + v\n",
    "                    probability = (bigrams.get(ngram, 0) + 1) / denominator\n",
    "                    smoothed_bigrams[ngram] = probability\n",
    "                    \n",
    "        return smoothed_bigrams\n",
    "    \n",
    "    def good_turing(self, bigrams, unigrams, d):\n",
    "        smoothed_bigrams = {}\n",
    "        for sentence in self.sentence:\n",
    "            words = sentence.split()\n",
    "            for index in range(len(words) - 1):\n",
    "                ngram = words[index] + ' ' + words[index + 1]\n",
    "                #print(ngram)\n",
    "                if ngram not in bigrams:\n",
    "                    denominator = unigrams.get(words[index], 0) + v\n",
    "                    probability = (bigrams.get(ngram, 0) - d) / denominator\n",
    "                    smoothed_bigrams[ngram] = probability\n",
    "                    \n",
    "        return smoothed_bigrams\n",
    "            \n",
    "    \n",
    "    def add_one_perplexity(self, add_one_mle):\n",
    "        probability = 1\n",
    "        num_words = sum([len(sentence.split()) for sentence in self.sentences])\n",
    "        num_words = 2\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                ngram = ' '.join(words[i : i + 2])\n",
    "                mle = add_one_mle.get(ngram, 1)\n",
    "                probability *= (1.0 / mle)\n",
    "                probability = (probability ** (1.0 / num_words))\n",
    "                \n",
    "        return probability\n",
    "    \n",
    "    def good_turing_perplexity(self, good_turing_mle):\n",
    "        probability = 1\n",
    "        num_words = sum([len(sentence.split()) for sentence in self.sentences])\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                ngram = ' '.join(words[i : i + 2])\n",
    "                mle = good_turing_mle.get(ngram, 1)\n",
    "                probability *= (1.0 / mle)\n",
    "                probability = (probability ** (1.0 / num_words))\n",
    "                \n",
    "        return probability\n",
    "\n",
    "perplexity = Perplexity(sentences_test)\n",
    "pp_one = perplexity.add_one_perplexity(new_bigrams_mle)\n",
    "print(\"Perplexity for add-1:\",pp_one)\n",
    "pp_good_turing = perplexity.good_turing_perplexity(good_turing_smoothing[1])\n",
    "print(\"Perplexity for Good-Turing:\",pp_good_turing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above numbers, PP(add-1) > PP(Good-Turing). Thus Good-Turing performs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
