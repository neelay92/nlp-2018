{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment $1$\n",
    "\n",
    "*Neelay Upadhyaya __18210053__*\n",
    "\n",
    "*M.Tech CSE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Task\n",
    "\n",
    "- Download any one of the text dataset mentioned in the previous lecture. \n",
    "\n",
    "- Compute tokens, types, and TTR.\n",
    "\n",
    "- Plot Zipf’s law and check if Zipf’s law holds true for meanings and lengths. When and when not?\n",
    "\n",
    "- Plot Heaps’ law. Fit a curve and report the estimated K and β values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radio Button for dataset.\n",
    "##### By default it's Mark Twain's Tom Sawyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "radio = widgets.RadioButtons(\n",
    "    options=['Tom Sawyer', 'Shakespeare'],\n",
    "    description='Select Dataset:'\n",
    ")\n",
    "display(radio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use nltk for tokenizing\n",
    "# so let's import\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what dataset have we selected?\n",
    "dataset = radio.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset selected: \" + dataset)\n",
    "\n",
    "# open the corresponding dataset file\n",
    "file = open(dataset.lower().replace(' ', '_') + \".txt\")\n",
    "\n",
    "# tokenize\n",
    "tokens = regexp_tokenize(file.read(), \"[\\w']+\")\n",
    "\n",
    "# normalize the data to lowercase\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the tokens\n",
    "token_length = len(tokens)\n",
    "print(\"Tokens: \" + str(token_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Length of types\n",
    "type_length = len(sorted(set(tokens)))\n",
    "print(\"Types: \" + str(type_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Type / Token Ratio $(TTR)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type / token ratio\n",
    "ttr = type_length / token_length\n",
    "print(\"TTR: \" + str(ttr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now map tokens to their frequencies\n",
    "token_dict = {}\n",
    "for token in tokens:\n",
    "    # if token doesn't exist, return 0\n",
    "    freq = token_dict.get(token, 0)\n",
    "    token_dict[token] = freq + 1\n",
    "\n",
    "# sort the token_dict according to the frequency in descending order\n",
    "token_list = sorted(list(token_dict.items()), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional if pandas is not installed.\n",
    "# Pandas is used to show the data in a table.\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "\n",
    "# we will show 30 most frequent tokens\n",
    "most_common = token_list[:30]\n",
    "data_frame = panda.DataFrame(most_common, columns=['word', 'frequency'])\n",
    "\n",
    "# ranking starts from 1\n",
    "data_frame.index = data_frame.index + 1\n",
    "\n",
    "# Index column name\n",
    "data_frame.columns.name = \"rank\"\n",
    "\n",
    "display(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law###\n",
    "\n",
    "**A relationship between the frequency of a word ( $f$ ) and its position in the list (its rank $r$).**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### $f \\propto \\frac{1}{r}$ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting frequency distribution\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(tokens)\n",
    "freq_dist.plot(30, title=\"Zipf's Law for \" + dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above figure, we have $30$ samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the whole graph\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.plot([r for r in range(1, len(token_list) + 1)], [i[1] for i in token_list])\n",
    "\n",
    "pyplot.xlabel('Rank(r)')\n",
    "pyplot.ylabel('Frequency(f)')\n",
    "pyplot.title(\"Zipf's Law for \" + dataset)\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above figure, we plot the whole Graph**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation: Word length and word frequency ###\n",
    "\n",
    "**Word frequency ( $f$ ) is inversely proportional to their length ( $l$ ).**\n",
    "\n",
    "---\n",
    "\n",
    "### $l \\propto \\frac{1}{f} $###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all lengths\n",
    "lengths = [len(w) for w in tokens]\n",
    "\n",
    "length_freq_dist = FreqDist(lengths)\n",
    "\n",
    "length_freq_dist.plot(title=\"Zipf's Law for \" + dataset)\n",
    "\n",
    "length_freq_dist.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, it is clear that relation between $l$ and $f$ does not hold for words of length $1$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation: Number of meanings and word frequency ###\n",
    "\n",
    "**The number of meanings ( $m$ ) of a word obeys the law:**\n",
    "\n",
    "---\n",
    "\n",
    "### $ m \\propto \\sqrt{f} $ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning_dict = {\n",
    "    'chronicle': [token_dict.get('chronicle'), 2],\n",
    "    'kill': [token_dict.get('kill'), 5],\n",
    "    'dainty': [token_dict.get('dainty'), 3],\n",
    "    'across': [token_dict.get('across'), 3],\n",
    "    'place': [token_dict.get('place'), 8],\n",
    "    'back': [token_dict.get('back'), 13],\n",
    "    'keep': [token_dict.get('keep'), 7],\n",
    "    'well': [token_dict.get('well'), 12],\n",
    "    'will': [token_dict.get('will'), 10],\n",
    "    'last': [token_dict.get('last'), 9],\n",
    "    'get': [token_dict.get('get'), 10],\n",
    "    'do': [token_dict.get('do'), 14],\n",
    "    'good': [token_dict.get('good'), 11],\n",
    "    'like': [token_dict.get('like'), 11],\n",
    "    \n",
    "}\n",
    "pyplot.plot(*zip(*sorted(list(meaning_dict.values()))))\n",
    "\n",
    "pyplot.xlabel('Frequency(f)')\n",
    "pyplot.ylabel('Meanings(m)')\n",
    "pyplot.title(\"Zipf's Law for \" + dataset)\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "for key, value in meaning_dict.items():\n",
    "    print(\"Token:\" + key + \", Frequency: \" + str(value[0]) + \", Meanings: \" + str(value[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heap's Law ###\n",
    "\n",
    "Let $\\left |  V \\right |$ be the size of vocabulary and $N$ be the number of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### $\\left |  V \\right | = K N^{\\beta}$ ####\n",
    "\n",
    "\n",
    "$ \\Rightarrow \\log{\\left |  V \\right |} = \\log{K} + \\beta.\\log{N}$\n",
    "\n",
    "\n",
    "\n",
    "$ \\Rightarrow y = c + m.x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "from numpy import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "token_n = set()\n",
    "heaps_x_y = []\n",
    "log_heaps_x_y = []\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    token_n.add(token)\n",
    "    log_heaps_x_y.append((log(i + 1), log(len(token_n))))\n",
    "    heaps_x_y.append(((i + 1), len(token_n)))\n",
    "\n",
    "\n",
    "plt.plot(*zip(*sorted(heaps_x_y)))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "slope, intercept = polyfit(*zip(*sorted(log_heaps_x_y)), 1)\n",
    "\n",
    "print(\"Value of K: \" + str(exp(intercept)))\n",
    "print(\"Value of B: \" + str(slope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we have,\n",
    "- $K \\approx  10-100$\n",
    "- $\\beta \\approx  0.4 - 0.6$ (roughly square root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
